wasp {
  hdfs-homedir = "hdfs://"${HOSTNAME}":9000/user/root/"
  systempipegraphs.start = false # whether to automatically start system pipegraphs
  systemproducers.start = false # whether to automatically start system producers
  darwinConnector = "mongo" #possible value is hbase, postgres, mongo
  mongo {
    address = "mongodb://"${HOSTNAME}":27017"
    collection-prefix = "docker-namespace"
  }
  rest {
    server {
      hostname = ${HOSTNAME}
    }
  }
  kafka {
    connections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 9092
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zookeeperConnections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
  }

  solrcloud {
    zookeeperConnections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
  }

  spark-streaming {
    master {
      host = "yarn"
    }
    driver-conf {
      submit-deploy-mode = "client"
      driver-hostname = ${HOSTNAME}
      driver-bind-address = ${HOSTNAME}
    }
    executor-cores = 2
    executor-memory = "2g"
    executor-instances = 1
    additional-jars-path = "/code/consumers-spark/lib"
    yarn-jar = "hdfs://"${HOSTNAME}":9000/user/root/spark2/lib/*"
    kryo-serializer {
      registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
        "",
      strict = false
    }
    checkpoint-dir =  "hdfs://"${HOSTNAME}":9000/user/root/checkpoint/"
    others = [
      {"spark.yarn.stagingDir" = "hdfs://"${HOSTNAME}":9000/user/root/.sparkStaging/"}
      {"spark.sql.shuffle.partitions" = "1"}
      { "spark.executor.extraClassPath" = "./org.codehaus.jackson.jackson-core-asl-1.9.13.jar:./org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar"}
      {"spark.files": "file://"${wasp.spark-streaming.additional-jars-path}"/org.codehaus.jackson.jackson-core-asl-1.9.13.jar,file://"${wasp.spark-streaming.additional-jars-path}"/org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar"}
      {"spark.yarn.dist.files": "file://"${wasp.spark-streaming.additional-jars-path}"/org.codehaus.jackson.jackson-core-asl-1.9.13.jar,file://"${wasp.spark-streaming.additional-jars-path}"/org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar"}
    ]
  }

  spark-batch {
    master {
      host = "yarn"
    }
    driver-conf {
      submit-deploy-mode = "client"
      driver-hostname = ${HOSTNAME}
      driver-bind-address = ${HOSTNAME}
    }
    executor-cores = 2
    executor-memory = "1g"
    executor-instances = 1
    additional-jars-path = "file:///code/consumers-spark/lib"
    yarn-jar = "hdfs://"${HOSTNAME}":9000/user/root/spark2/lib/*"
    others = [
      {"spark.yarn.stagingDir" = "hdfs://"${HOSTNAME}":9000/user/root/.sparkStaging/"},
      {"spark.sql.shuffle.partitions" = "1"}
    ]
  }

  avroSchemaManager {
    wasp-manages-darwin-connectors-conf = true

    ##hbase-conf
    darwin {
      connector = "hbase"
      namespace = "AVRO"            #optional
      table = "SCHEMA_REPOSITORY"   #optional
      type = "cached_eager"
      endianness = "LITTLE_ENDIAN"
      isSecure = false
      createTable = true
      #hbaseSite =    required if wasp-manages-darwin-connectors-conf= false
      #coreSite =     required if wasp-manages-darwin-connectors-conf= false
      #isSecure =     required if wasp-manages-darwin-connectors-conf= false
      #principal =    required if wasp-manages-darwin-connectors-conf= false
      #keytabPath =   required if wasp-manages-darwin-connectors-conf= false
    }
  }
}
