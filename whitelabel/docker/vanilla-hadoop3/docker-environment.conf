wasp {
  plugin {
    microservice-catalog {
      catalog-class: "it.agilelab.bigdata.wasp.whitelabel.consumers.spark.catalog.EntityCatalog"
    }
  }
  hdfs-homedir = "hdfs://"${HOSTNAME}":9000/user/root/"
  systempipegraphs.start = false # whether to automatically start system pipegraphs
  systemproducers.start = false # whether to automatically start system producers
  darwinConnector = "mongo" #possible value is hbase, postgres, mongo
  mongo {
    address = "mongodb://"${HOSTNAME}":27017"
    collection-prefix = "namespace-prefix"
  }
  rest {
    server {
      hostname = ${HOSTNAME}
    }
  }
  kafka {
    connections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 9092
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    zookeeperConnections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
  }

  solrcloud {
    zookeeperConnections = [{
      protocol = ""
      host = ${HOSTNAME}
      port = 2181
      timeout = ${wasp.services-timeout-millis}
      metadata = []
    }]
    #zkChRoot =
  }

  spark-streaming {

    master {
      host = "yarn"
    }
    driver-conf {
      submit-deploy-mode = "client"
      driver-hostname = ${HOSTNAME}
      driver-bind-address = ${HOSTNAME}
    }
    executor-cores = 2
    executor-memory = "2g"
    executor-instances = 1
    additional-jars-path = "/code/consumers-spark/lib"
    yarn-jar = "hdfs://"${HOSTNAME}":9000/user/root/spark2/lib/*"
    kryo-serializer {
      registrators = # comma-separated custom-KryoRegistrator list of fully qualified names
        "",
      strict = false
    }
    checkpoint-dir = "hdfs://"${HOSTNAME}":9000/user/root/checkpoint/"
    others = [
      {"spark.sql.catalogImplementation" = "hive"}
      {"spark.sql.hive.metastore.version" = "2.3.3"}
      {"spark.hadoop.datanucleus.schema.autoCreateTables" = "true"}
      {"spark.hadoop.hive.metastore.schema.verification" = "false"}
      {"spark.sql.hive.metastore.jars" = "maven"}
      {"spark.yarn.stagingDir" = "hdfs://"${HOSTNAME}":9000/user/root/.sparkStaging/"}
      {"spark.sql.shuffle.partitions" = "1"}
      {"spark.executor.extraClassPath" = "./org.codehaus.jackson.jackson-core-asl-1.9.13.jar:./org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar"}
      {"spark.files": "file:///code/consumers-spark/lib/org.codehaus.jackson.jackson-core-asl-1.9.13.jar,file:///code/consumers-spark/lib/org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar"}
      {"spark.yarn.dist.files": "file:///code/consumers-spark/lib/org.codehaus.jackson.jackson-core-asl-1.9.13.jar,file:///code/consumers-spark/lib/org.codehaus.jackson.jackson-mapper-asl-1.9.13.jar"}
      {"spark.hadoop.fs.s3a.impl" = "org.apache.hadoop.fs.s3a.S3AFileSystem"}
      {"spark.hadoop.fs.s3a.path.style.access" = "true"}
      {"spark.hadoop.fs.s3a.aws.credentials.provider" = "it.agilelab.bigdata.wasp.aws.auth.v2.PlacementAwareCredentialsProvider"}
      {"spark.hadoop.fs.s3a.assumed.role.credentials.provider" = "it.agilelab.bigdata.wasp.whitelabel.consumers.spark.aws.MinioCredentialsProvider"}
      {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.storage" = "hdfs://"${HOSTNAME}":9000/tmp/aws"}
      {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.checkpointbucket" = "hdfs://"${HOSTNAME}":9000/user/root/checkpoint/"}
      {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.renewmillis" = "60000"}
      {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.delegate" = "org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider"}
      {"spark.hadoop.fs.s3a.assumed.role.arn" = "arn:xxx:xxx:xxx:xxxx"}
      {"spark.hadoop.fs.s3a.endpoint" = "http://"${MINIO_ENDPOINT}""}
      {"spark.hadoop.fs.s3a.assumed.role.sts.endpoint" = "http://"${MINIO_ENDPOINT}""}
      {"spark.hadoop.fs.s3a.assumed.role.sts.endpoint.region" = "eu-central-1"}
      #  {"spark.hadoop.fs.s3a.aws.credentials.provider" = "it.agilelab.bigdata.wasp.aws.auth.v2.PlacementAwareCredentialsProvider"}
      #  {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.storage" = "hdfs://"${HOSTNAME}":9000/user/root/aws"}
      #   {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.renewmillis" = "10000"}
      #   {"spark.hadoop.it.agilelab.bigdata.wasp.aws.auth.delegate" = "org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider"}

      #   {"spark.hadoop.fs.s3a.endpoint": "s3.eu-central-1.amazonaws.com" }
    ]
  }

  spark-batch {
    master {
      host = "yarn"
    }
    driver-conf {
      submit-deploy-mode = "client"
      driver-hostname = ${HOSTNAME}
      driver-bind-address = ${HOSTNAME}
    }
    executor-cores = 2
    executor-memory = "1g"
    executor-instances = 1
    additional-jars-path = "/code/consumers-spark/lib"
    yarn-jar = "hdfs://"${HOSTNAME}":9000/user/root/spark2/lib/*"
    others = [
      {"spark.yarn.stagingDir" = "hdfs://"${HOSTNAME}":9000/user/root/.sparkStaging/"},
      {"spark.sql.shuffle.partitions" = "1"}
    ]
  }

  avroSchemaManager {
    wasp-manages-darwin-connectors-conf = true

    ##hbase-conf
    darwin {
      connector = "hbase"
      namespace = "AVRO"            #optional
      table = "SCHEMA_REPOSITORY"   #optional
      type = "cached_eager"
      endianness = "LITTLE_ENDIAN"
      isSecure = false
      createTable = true
      #hbaseSite =    required if wasp-manages-darwin-connectors-conf= false
      #coreSite =     required if wasp-manages-darwin-connectors-conf= false
      #isSecure =     required if wasp-manages-darwin-connectors-conf= false
      #principal =    required if wasp-manages-darwin-connectors-conf= false
      #keytabPath =   required if wasp-manages-darwin-connectors-conf= false
    }
  }
}
